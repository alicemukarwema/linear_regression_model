{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d2eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89234ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "def analyze_and_preprocess(df, target_column):\n",
    "    print(\"\\n==== DATA ANALYSIS ====\")\n",
    "    print(\"Initial data shape:\", df.shape)\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    # Drop any rows with missing target values\n",
    "    df = df.dropna(subset=[target_column])\n",
    "\n",
    "    # Convert boolean columns to int\n",
    "    bool_cols = df.select_dtypes(include='bool').columns\n",
    "    df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "    # Identify numeric and categorical features\n",
    "    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    numeric_features.remove(target_column)\n",
    "\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(\"\\nNumeric features:\", numeric_features)\n",
    "    print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "    return df, numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907937b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(df, numeric_features, categorical_features, target_column):\n",
    "    print(\"\\n==== CREATING VISUALIZATIONS ====\")\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df[numeric_features + [target_column]].corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Distribution of target variable\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(df[target_column], kde=True)\n",
    "    plt.title(f'Distribution of {target_column}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/target_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Pairplot for numeric features\n",
    "    if len(numeric_features) > 0:\n",
    "        sns.pairplot(df[numeric_features + [target_column]], diag_kind='kde')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/numeric_pairplot.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Boxplots for categorical features\n",
    "    for feature in categorical_features:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=feature, y=target_column, data=df)\n",
    "        plt.title(f'{target_column} by {feature}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'visualizations/{feature}_boxplot.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c8b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_models(X, y, numeric_features, categorical_features):\n",
    "    print(\"\\n==== MODEL TRAINING ====\")\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'SGD Regression': SGDRegressor(max_iter=1000, tol=1e-3),\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_score = float('inf')\n",
    "    best_model_name = None\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', model)])\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': pipeline,\n",
    "            'mse': mse,\n",
    "            'r2': r2\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "        \n",
    "        # Track best model\n",
    "        if mse < best_score:\n",
    "            best_score = mse\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name} with MSE: {best_score:.4f}\")\n",
    "    return models, results, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f82d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_linear_regression(models, X, y, numeric_features, target_column):\n",
    "    if 'Linear Regression' in models:\n",
    "        print(\"\\n==== LINEAR REGRESSION VISUALIZATION ====\")\n",
    "        \n",
    "        # Get the first numeric feature for visualization\n",
    "        if len(numeric_features) > 0:\n",
    "            feature = numeric_features[0]\n",
    "            \n",
    "            # Simple linear regression for visualization\n",
    "            X_vis = X[[feature]]\n",
    "            y_vis = y\n",
    "            \n",
    "            # Train simple model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_vis, y_vis)\n",
    "            \n",
    "            # Predictions\n",
    "            x_range = np.linspace(X_vis.min(), X_vis.max(), 100)\n",
    "            y_pred = model.predict(x_range.reshape(-1, 1))\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(X_vis, y_vis, alpha=0.5, label='Actual Data')\n",
    "            plt.plot(x_range, y_pred, color='red', linewidth=2, label='Regression Line')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel(target_column)\n",
    "            plt.title(f'Simple Linear Regression: {target_column} vs {feature}')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('visualizations/linear_regression_plot.png')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fee7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_best_model(best_model_name, X, y, numeric_features, categorical_features):\n",
    "    print(f\"\\n==== TUNING BEST MODEL: {best_model_name} ====\")\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    # Define parameter grids for different models\n",
    "    param_grids = {\n",
    "        'Linear Regression': {},\n",
    "        'SGD Regression': {\n",
    "            'regressor__alpha': [0.0001, 0.001, 0.01],\n",
    "            'regressor__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'regressor__learning_rate': ['constant', 'optimal', 'invscaling']\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'regressor__max_depth': [None, 5, 10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__max_depth': [None, 10, 20],\n",
    "            'regressor__min_samples_split': [2, 5]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Select the appropriate model\n",
    "    model_map = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'SGD Regression': SGDRegressor(max_iter=1000, tol=1e-3),\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model_map[best_model_name])])\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=param_grids[best_model_name],\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = best_model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Training MSE: {mse:.4f}\")\n",
    "    print(f\"Training R2: {r2:.4f}\")\n",
    "    \n",
    "    return best_model, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3c0bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, numeric_features, categorical_features):\n",
    "    print(\"\\n==== FEATURE IMPORTANCE ====\")\n",
    "    \n",
    "    # Check if model is a pipeline\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        # Get feature names\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        \n",
    "        # Get categorical feature names after one-hot encoding\n",
    "        if len(categorical_features) > 0:\n",
    "            ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "            cat_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "        else:\n",
    "            cat_feature_names = []\n",
    "        \n",
    "        all_feature_names = numeric_features + list(cat_feature_names)\n",
    "        \n",
    "        # Get feature importance based on model type\n",
    "        if hasattr(model.named_steps['regressor'], 'coef_'):\n",
    "            # Linear models\n",
    "            importance = model.named_steps['regressor'].coef_\n",
    "            plt.barh(all_feature_names, importance)\n",
    "            plt.title('Feature Coefficients')\n",
    "        elif hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "            # Tree-based models\n",
    "            importance = model.named_steps['regressor'].feature_importances_\n",
    "            plt.barh(all_feature_names, importance)\n",
    "            plt.title('Feature Importances')\n",
    "        else:\n",
    "            print(\"Feature importance not available for this model type.\")\n",
    "            return\n",
    "        \n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/feature_importance.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9380a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model, numeric_features, categorical_features, target_column):\n",
    "    print(\"\\n==== SAVING BEST MODEL ====\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'numeric_features': numeric_features,\n",
    "        'categorical_features': categorical_features,\n",
    "        'target_column': target_column\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, 'models/best_model.pkl')\n",
    "    print(\"Model saved to 'models/best_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733c0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== CROP YIELD PREDICTION MODEL ====\n",
      "Loading data from dataset file\n",
      "\n",
      "==== DATA ANALYSIS ====\n",
      "Initial data shape: (1000000, 10)\n",
      "\n",
      "Data types:\n",
      "Region                     object\n",
      "Soil_Type                  object\n",
      "Crop                       object\n",
      "Rainfall_mm               float64\n",
      "Temperature_Celsius       float64\n",
      "Fertilizer_Used              bool\n",
      "Irrigation_Used              bool\n",
      "Weather_Condition          object\n",
      "Days_to_Harvest             int64\n",
      "Yield_tons_per_hectare    float64\n",
      "dtype: object\n",
      "\n",
      "Numeric features: ['Rainfall_mm', 'Temperature_Celsius', 'Fertilizer_Used', 'Irrigation_Used', 'Days_to_Harvest']\n",
      "Categorical features: ['Region', 'Soil_Type', 'Crop', 'Weather_Condition']\n",
      "\n",
      "==== CREATING VISUALIZATIONS ====\n",
      "\n",
      "==== MODEL TRAINING ====\n",
      "Linear Regression - MSE: 0.2508, R2: 0.9131\n",
      "SGD Regression - MSE: 0.2510, R2: 0.9130\n",
      "Decision Tree - MSE: 0.5320, R2: 0.8156\n",
      "Random Forest - MSE: 0.2663, R2: 0.9077\n",
      "\n",
      "Best model: Linear Regression with MSE: 0.2508\n",
      "\n",
      "==== LINEAR REGRESSION VISUALIZATION ====\n",
      "\n",
      "==== TUNING BEST MODEL: Linear Regression ====\n",
      "Best parameters: {}\n",
      "Training MSE: 0.2505\n",
      "Training R2: 0.9130\n",
      "\n",
      "==== PREDICTION ON SINGLE TEST EXAMPLE ====\n",
      "Example features:\n",
      "  Region: West\n",
      "  Soil_Type: Silt\n",
      "  Crop: Cotton\n",
      "  Rainfall_mm: 714.854403349598\n",
      "  Temperature_Celsius: 23.875872058997384\n",
      "  Fertilizer_Used: 0\n",
      "  Irrigation_Used: 0\n",
      "  Weather_Condition: Sunny\n",
      "  Days_to_Harvest: 120\n",
      "\n",
      "Actual Yield_tons_per_hectare: 3.8409878810017286\n",
      "Predicted Yield_tons_per_hectare: 4.05\n",
      "Absolute error: 0.21\n",
      "\n",
      "==== FEATURE IMPORTANCE ====\n",
      "\n",
      "==== SAVING BEST MODEL ====\n",
      "Model saved to 'models/best_model.pkl'\n",
      "\n",
      "==== MODEL TRAINING COMPLETE ====\n",
      "Best model: Tuned Linear Regression\n",
      "Test MSE: 0.25\n",
      "R² Score: 0.91\n",
      "\n",
      "All visualizations saved in 'visualizations' folder\n",
      "Model saved in 'models' folder\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"==== CROP YIELD PREDICTION MODEL ====\")\n",
    "    \n",
    "    # Load data directly from CSV\n",
    "    print(\"Loading data from dataset file\")\n",
    "    df = pd.read_csv('african_crop_yield.csv')\n",
    "    \n",
    "    # Set target column\n",
    "    target_column = 'Yield_tons_per_hectare'\n",
    "    \n",
    "    # Analyze and preprocess data\n",
    "    df, numeric_features, categorical_features = analyze_and_preprocess(df, target_column)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(df, numeric_features, categorical_features, target_column)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Build and train models\n",
    "    models, results, best_model_name = build_and_train_models(X, y, numeric_features, categorical_features)\n",
    "        \n",
    "    # Create train/test split for visualizations\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Visualize linear regression results\n",
    "    visualize_linear_regression(models, X, y, numeric_features, target_column)\n",
    "    \n",
    "    # Tune the best model\n",
    "    best_model, test_mse, r2 = tune_best_model(best_model_name, X, y, numeric_features, categorical_features)\n",
    "    \n",
    "    # Make prediction on a single test data point\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    if len(X_test) > 0:\n",
    "        # Select a single test example\n",
    "        single_example = X_test.iloc[0:1]\n",
    "        actual_value = y_test.iloc[0]\n",
    "        \n",
    "        # Make prediction using the best model\n",
    "        predicted_value = best_model.predict(single_example)[0]\n",
    "        \n",
    "        print(f\"\\n==== PREDICTION ON SINGLE TEST EXAMPLE ====\")\n",
    "        print(f\"Example features:\")\n",
    "        for col in single_example.columns:\n",
    "            print(f\"  {col}: {single_example[col].iloc[0]}\")\n",
    "            \n",
    "        print(f\"\\nActual {target_column}: {actual_value}\")\n",
    "        print(f\"Predicted {target_column}: {predicted_value:.2f}\")\n",
    "        print(f\"Absolute error: {abs(actual_value - predicted_value):.2f}\")\n",
    "    \n",
    "    # Analyze feature importance for the best model\n",
    "    feature_importance(best_model, numeric_features, categorical_features)\n",
    "    \n",
    "    # Save the best model\n",
    "    save_best_model(best_model, numeric_features, categorical_features, target_column)\n",
    "    \n",
    "    print(\"\\n==== MODEL TRAINING COMPLETE ====\")\n",
    "    print(f\"Best model: Tuned {best_model_name}\")\n",
    "    print(f\"Test MSE: {test_mse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.2f}\")\n",
    "    print(\"\\nAll visualizations saved in 'visualizations' folder\")\n",
    "    print(\"Model saved in 'models' folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
